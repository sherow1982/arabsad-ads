#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÙƒØ±Ø¨Øª Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù€ SEO ÙˆLocal Business Ùˆ Content Optimization
Ø±ÙŠØ¨Ùˆ: arabsad-ads (Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨)
Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ ÙƒØ§Ù…Ù„Ø© + ÙƒÙ„ Ù…Ø¯Ù†Ù‡Ø§
"""

import sys
import re
import json
from pathlib import Path
from datetime import datetime, timedelta

# ================== Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ ÙƒØ§Ù…Ù„Ø© ==================

GULF_COUNTRIES = {
    "SA": {
        "name": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "arabic_name": "Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "lat": 24.7136,
        "lng": 46.6753,
        "cities": {
            "Ø§Ù„Ø±ÙŠØ§Ø¶": {"lat": 24.7136, "lng": 46.6753},
            "Ø¬Ø¯Ø©": {"lat": 21.5485, "lng": 39.1721},
            "Ø§Ù„Ø¯Ù…Ø§Ù…": {"lat": 26.3989, "lng": 50.2048},
            "Ø§Ù„Ø®Ø¨Ø±": {"lat": 26.2156, "lng": 50.2106},
            "Ø§Ù„Ù‚Ø·ÙŠÙ": {"lat": 26.1801, "lng": 50.0157},
            "Ù…ÙƒØ©": {"lat": 21.4225, "lng": 39.8262},
            "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©": {"lat": 24.4647, "lng": 39.6074},
            "Ø§Ù„Ø·Ø§Ø¦Ù": {"lat": 21.2745, "lng": 40.4158},
            "ØªØ¨ÙˆÙƒ": {"lat": 28.3852, "lng": 36.5627},
            "Ø£Ø¨Ù‡Ø§": {"lat": 18.2155, "lng": 42.5054},
            "Ø¬ÙŠØ²Ø§Ù†": {"lat": 16.8892, "lng": 42.5521},
            "Ù†Ø¬Ø±Ø§Ù†": {"lat": 17.6927, "lng": 44.1860},
            "Ø­ÙØ± Ø§Ù„Ø¨Ø§Ø·Ù†": {"lat": 28.4347, "lng": 45.3569},
        },
        "keywords": [
            "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
            "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø±ÙŠØ§Ø¶",
            "Google Ads Ø¬Ø¯Ø©",
            "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
            "Facebook Ads Ø§Ù„Ø¯Ù…Ø§Ù…",
            "SEO Ù…ÙƒØ©",
            "ØªØµÙ…ÙŠÙ… Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        ]
    },
    "AE": {
        "name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
        "arabic_name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©",
        "lat": 23.4241,
        "lng": 53.8478,
        "cities": {
            "Ø¯Ø¨ÙŠ": {"lat": 25.2048, "lng": 55.2708},
            "Ø£Ø¨ÙˆØ¸Ø¨ÙŠ": {"lat": 24.4539, "lng": 54.3773},
            "Ø§Ù„Ø´Ø§Ø±Ù‚Ø©": {"lat": 25.3548, "lng": 55.3944},
            "Ø¹Ø¬Ù…Ø§Ù†": {"lat": 25.3986, "lng": 55.4501},
            "Ø£Ù… Ø§Ù„Ù‚ÙŠÙˆÙŠÙ†": {"lat": 25.5645, "lng": 55.5597},
            "Ø±Ø£Ø³ Ø§Ù„Ø®ÙŠÙ…Ø©": {"lat": 25.7482, "lng": 55.9754},
            "Ø§Ù„ÙØ¬ÙŠØ±Ø©": {"lat": 25.1242, "lng": 56.3540},
        },
        "keywords": [
            "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
            "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø¯Ø¨ÙŠ",
            "Google Ads Ø£Ø¨ÙˆØ¸Ø¨ÙŠ",
            "SEO Ø§Ù„Ø´Ø§Ø±Ù‚Ø©",
            "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
            "Facebook Ads Ø¯Ø¨ÙŠ",
            "ØªØµÙ…ÙŠÙ… Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
            "Ø®Ø¯Ù…Ø§Øª ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø¯Ø¨ÙŠ",
        ]
    },
    "KW": {
        "name": "Ø§Ù„ÙƒÙˆÙŠØª",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ø§Ù„ÙƒÙˆÙŠØª",
        "lat": 29.3759,
        "lng": 47.9774,
        "cities": {
            "Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„ÙƒÙˆÙŠØª": {"lat": 29.3759, "lng": 47.9774},
            "Ø§Ù„Ø£Ø­Ù…Ø¯ÙŠ": {"lat": 29.1118, "lng": 47.6929},
            "Ø§Ù„Ø¬Ù‡Ø±Ø§Ø¡": {"lat": 29.4444, "lng": 47.6804},
            "Ø§Ù„ÙØ±ÙˆØ§Ù†ÙŠØ©": {"lat": 29.2269, "lng": 47.8558},
            "Ø­ÙˆÙ„ÙŠ": {"lat": 29.3621, "lng": 47.9825},
            "Ù…Ø¨Ø§Ø±Ùƒ Ø§Ù„ÙƒØ¨ÙŠØ±": {"lat": 29.0269, "lng": 47.7373},
            "Ø§Ù„Ø¹Ø§ØµÙ…Ø©": {"lat": 29.3759, "lng": 47.9774},
        },
        "keywords": [
            "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„ÙƒÙˆÙŠØª",
            "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„ÙƒÙˆÙŠØª",
            "Google Ads Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„ÙƒÙˆÙŠØª",
            "SEO Ø§Ù„ÙƒÙˆÙŠØª",
            "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ÙƒÙˆÙŠØª",
            "Facebook Ads Ø§Ù„ÙƒÙˆÙŠØª",
            "ØªØµÙ…ÙŠÙ… Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙƒÙˆÙŠØª",
        ]
    },
    "QA": {
        "name": "Ù‚Ø·Ø±",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ù‚Ø·Ø±",
        "lat": 25.2854,
        "lng": 51.5310,
        "cities": {
            "Ø§Ù„Ø¯ÙˆØ­Ø©": {"lat": 25.2854, "lng": 51.5310},
            "Ø§Ù„Ø±ÙŠØ§Ù†": {"lat": 25.3548, "lng": 51.5342},
            "Ø§Ù„ÙˆÙƒØ±Ø©": {"lat": 25.1673, "lng": 51.6286},
            "Ø§Ù„Ø®ÙˆØ±": {"lat": 25.6753, "lng": 51.4805},
            "Ø£Ù… ØµÙ„Ø§Ù„": {"lat": 25.4167, "lng": 51.5000},
            "Ø§Ù„Ø´Ù…Ø§Ù„": {"lat": 25.8500, "lng": 51.2500},
        },
        "keywords": [
            "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù‚Ø·Ø±",
            "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¯ÙˆØ­Ø©",
            "Google Ads Ù‚Ø·Ø±",
            "SEO Ø§Ù„Ø¯ÙˆØ­Ø©",
            "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ù‚Ø·Ø±",
            "Facebook Ads Ø§Ù„Ø¯ÙˆØ­Ø©",
            "ØªØµÙ…ÙŠÙ… Ù…ÙˆØ§Ù‚Ø¹ Ù‚Ø·Ø±",
        ]
    },
    "BH": {
        "name": "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "arabic_name": "Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "lat": 26.0667,
        "lng": 50.5577,
        "cities": {
            "Ø§Ù„Ù…Ù†Ø§Ù…Ø©": {"lat": 26.1290, "lng": 50.5826},
            "Ø§Ù„Ù…Ø­Ø±Ù‚": {"lat": 26.1667, "lng": 50.5833},
            "Ø§Ù„Ø±ÙØ§Ø¹": {"lat": 26.1333, "lng": 50.4167},
            "Ø§Ù„Ø¬ÙÙŠØ±": {"lat": 26.1778, "lng": 50.4389},
            "Ø³Ù„Ù…Ø§Ù† Ø¢Ø¨Ø§Ø¯": {"lat": 26.0833, "lng": 50.5000},
        },
        "keywords": [
            "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
            "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
            "Google Ads Ø§Ù„Ù…Ù†Ø§Ù…Ø©",
            "SEO Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
            "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
            "Facebook Ads Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
            "ØªØµÙ…ÙŠÙ… Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        ]
    },
    "OM": {
        "name": "Ø¹Ù…Ø§Ù†",
        "arabic_name": "Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†",
        "lat": 21.4735,
        "lng": 55.9754,
        "cities": {
            "Ù…Ø³Ù‚Ø·": {"lat": 21.4735, "lng": 55.9754},
            "ØµÙ„Ø§Ù„Ø©": {"lat": 17.0151, "lng": 54.0924},
            "ØµØ­Ø§Ø±": {"lat": 24.2795, "lng": 56.9366},
            "Ù†Ø²ÙˆÙ‰": {"lat": 22.9342, "lng": 57.5364},
            "Ø§Ù„Ø³ÙˆÙŠÙ‚": {"lat": 23.8069, "lng": 57.4074},
            "Ø´Ù†Ø§Øµ": {"lat": 24.7167, "lng": 56.7833},
            "Ù‡ÙŠÙ…Ø§Ø¡": {"lat": 24.2000, "lng": 56.6000},
        },
        "keywords": [
            "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø¹Ù…Ø§Ù†",
            "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ Ù…Ø³Ù‚Ø·",
            "Google Ads Ø¹Ù…Ø§Ù†",
            "SEO Ù…Ø³Ù‚Ø·",
            "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« Ø¹Ù…Ø§Ù†",
            "Facebook Ads Ø¹Ù…Ø§Ù†",
            "ØªØµÙ…ÙŠÙ… Ù…ÙˆØ§Ù‚Ø¹ Ø¹Ù…Ø§Ù†",
        ]
    },
}

GLOBAL_KEYWORDS = {
    "google_ads": [
        "Google Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„",
        "Ø­Ù…Ù„Ø§Øª Ø¬ÙˆØ¬Ù„",
        "Google Search Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¨Ø­Ø«",
        "Google Display Network",
        "GDN",
    ],
    "facebook_ads": [
        "Facebook Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ",
        "Instagram Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¥Ù†Ø³ØªØ¬Ø±Ø§Ù…",
        "Social Media Ads",
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„",
    ],
    "seo": [
        "SEO",
        "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«",
        "Search Engine Optimization",
        "Ø§Ù„ØªØ±ØªÙŠØ¨ ÙÙŠ Ø¬ÙˆØ¬Ù„",
        "Ù…Ø­Ø³Ù† Ø§Ù„Ø¨Ø­Ø«",
        "Link Building",
        "Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·",
    ],
    "web_design": [
        "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "Web Design",
        "ØªØµÙ…ÙŠÙ… Ù…ÙˆÙ‚Ø¹",
        "Website Design",
        "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "Web Development",
    ],
}

# ================== Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==================

def extract_title(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"""
    m = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt.split('|')[0].strip() if '|' in txt else txt
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    return "ØµÙØ­Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_description(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ"""
    m = re.search(r'<meta\s+name=["\']description["\']\s+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    m = re.search(r'<p[^>]*>([^<]+)</p>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt if len(txt) <= 155 else txt[:152] + "..."
    return "Ø®Ø¯Ù…Ø§Øª ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªÙ…ÙŠØ²Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_image(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©"""
    m = re.search(r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>', html, re.IGNORECASE)
    if m:
        src = m.group(1).strip()
        if src.startswith('http'):
            return src
        src = src.lstrip('./')
        return f"https://sherow1982.github.io/arabsad-ads/{src}"
    return "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"

def determine_page_type(file_path: Path) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©"""
    relative = str(file_path.relative_to(Path("."))).lower()
    if 'blog/articles' in relative:
        return 'article'
    elif 'blog' in relative:
        return 'blog'
    elif 'services' in relative:
        return 'service'
    elif 'cities' in relative:
        return 'city'
    return 'page'

def build_page_url(file_path: Path) -> str:
    """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø·"""
    relative_path = file_path.relative_to(Path("."))
    url_path = str(relative_path).replace("\\", "/")
    return f"https://sherow1982.github.io/arabsad-ads/{url_path}"

def extract_page_keywords(file_path: Path, title: str) -> list:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ keywords Ù…Ù† Ø§Ù„ØµÙØ­Ø© ÙˆØ¥Ø¶Ø§ÙØ© keywords Ø§Ù„Ø®Ù„ÙŠØ¬ ÙƒØ§Ù…Ù„Ø©"""
    keywords = []
    
    # Ø£Ø¶Ù keywords Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø©
    if 'google-ads' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["google_ads"])
    elif 'seo' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["seo"])
    elif 'social' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["facebook_ads"])
    elif 'website' in str(file_path).lower() or 'design' in str(file_path).lower():
        keywords.extend(GLOBAL_KEYWORDS["web_design"])
    
    # Ø£Ø¶Ù keywords ÙƒÙ„ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ ÙˆØ§Ù„Ù…Ø¯Ù†
    for country_code, country_data in GULF_COUNTRIES.items():
        keywords.extend(country_data["keywords"][:2])
        for city in list(country_data["cities"].keys())[:2]:
            keywords.append(f"ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ {city}")
    
    # Ø£Ø¶Ù Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    keywords.append(title)
    
    return list(set(keywords))[:20]

# ================== Schema ÙˆMeta ==================

def create_service_schema(title: str, image: str, url: str, description: str) -> str:
    """Service Schema"""
    import json
    
    area_served = []
    for country_code, country_data in GULF_COUNTRIES.items():
        area_served.append({"@type": "Country", "name": country_data['arabic_name']})
    
    schema = {
        "@context": "https://schema.org/",
        "@type": "Service",
        "name": title,
        "image": image,
        "description": description,
        "provider": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "url": "https://sherow1982.github.io/arabsad-ads/",
            "logo": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
            "telephone": "+201110760081"
        },
        "url": url,
        "areaServed": area_served,
        "priceRange": "$$-$$$",
        "potentialAction": {
            "@type": "ReserveAction",
            "target": {
                "@type": "EntryPoint",
                "urlTemplate": "https://wa.me/201110760081?text=Ø£Ø±ÙŠØ¯ Ø§Ø³ØªØ´Ø§Ø±Ø©"
            }
        }
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_article_schema(title: str, image: str, url: str, description: str, file_path: Path) -> str:
    """Article Schema"""
    import json
    try:
        date_modified = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
    except:
        date_modified = datetime.now().isoformat()
    
    schema = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": title,
        "image": image,
        "description": description,
        "datePublished": date_modified,
        "dateModified": date_modified,
        "author": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"
        },
        "publisher": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "logo": {
                "@type": "ImageObject",
                "url": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"
            }
        },
        "url": url
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_organization_schema() -> str:
    """Organization Schema"""
    import json
    schema = {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
        "alternateName": "ArabSad Digital Marketing",
        "image": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
        "description": "ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªØ®ØµØµØ© ÙÙŠ Google Ads ÙˆFacebook Ads ÙˆSEO ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "url": "https://sherow1982.github.io/arabsad-ads/",
        "telephone": "+201110760081",
        "email": "info@arabsad.com",
        "address": {
            "@type": "PostalAddress",
            "addressCountry": "EG",
            "addressRegion": "Ø§Ù„Ø¬ÙŠØ²Ø©",
            "addressLocality": "Ø­Ø¯Ø§Ø¦Ù‚ Ø£ÙƒØªÙˆØ¨Ø±"
        },
        "sameAs": [
            "https://www.facebook.com/arabsad",
            "https://www.twitter.com/arabsad",
            "https://www.instagram.com/arabsad"
        ]
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_local_business_schemas_all() -> list:
    """LocalBusiness Schema Ù„ÙƒÙ„ Ø¯ÙˆÙ„ ÙˆÙ…Ø¯Ù† Ø§Ù„Ø®Ù„ÙŠØ¬"""
    import json
    schemas = []
    
    for country_code, country_data in GULF_COUNTRIES.items():
        for city_name, city_coords in country_data["cities"].items():
            schema = {
                "@context": "https://schema.org",
                "@type": "LocalBusiness",
                "name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {city_name}",
                "alternateName": f"ArabSad {city_name}",
                "image": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
                "url": "https://sherow1982.github.io/arabsad-ads/",
                "telephone": "+201110760081",
                "email": "info@arabsad.com",
                "address": {
                    "@type": "PostalAddress",
                    "addressCountry": country_code,
                    "addressRegion": country_data['name'],
                    "addressLocality": city_name
                },
                "geo": {
                    "@type": "GeoCoordinates",
                    "latitude": city_coords['lat'],
                    "longitude": city_coords['lng']
                },
                "openingHours": "Su-Sa 08:00-23:00",
                "priceRange": "$$",
                "areaServed": [city_name, country_data['name']],
                "serviceArea": [
                    {
                        "@type": "City",
                        "name": city_name,
                        "areaServed": country_data['name']
                    }
                ]
            }
            schemas.append(json.dumps(schema, ensure_ascii=False, indent=2))
    
    return schemas

def create_breadcrumb_schema(file_path: Path) -> str:
    """Breadcrumb Schema"""
    import json
    relative = file_path.relative_to(Path("."))
    parts = relative.parts
    breadcrumb_items = []
    base_url = "https://sherow1982.github.io/arabsad-ads"
    
    breadcrumb_items.append({
        "@type": "ListItem",
        "position": 1,
        "name": "Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
        "item": base_url
    })
    
    current_path = ""
    for i, part in enumerate(parts[:-1], start=2):
        current_path += f"/{part}" if current_path else part
        name = part.replace('-', ' ').title()
        breadcrumb_items.append({
            "@type": "ListItem",
            "position": i,
            "name": name,
            "item": f"{base_url}/{current_path}"
        })
    
    schema = {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": breadcrumb_items
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_meta_tags(title: str, image: str, url: str, description: str, keywords: list) -> str:
    """Meta Tags Ù…Ø­Ø³Ù‘Ù†Ø© Ù…Ø¹ Keywords"""
    if len(description) > 155:
        desc_short = description[:152] + "..."
    else:
        desc_short = description
    
    title_clean = title.replace('"', '').replace("'", '')
    keywords_str = ", ".join(keywords[:15])
    
    meta = f"""
    <!-- SEO Meta Tags (Auto) -->
    <meta charset="UTF-8">
    <title>{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ | ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø®Ù„ÙŠØ¬</title>
    <meta name="description" content="{desc_short}">
    <meta name="keywords" content="{keywords_str}">
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="language" content="ar">
    <meta name="author" content="Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta name="geo.region" content="EG">
    <meta name="geo.placename" content="Ù…ØµØ±">
    <link rel="canonical" href="{url}">
    <link rel="alternate" hreflang="ar" href="{url}">
    <!-- Open Graph -->
    <meta property="og:title" content="{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta property="og:description" content="{desc_short}">
    <meta property="og:image" content="{image}">
    <meta property="og:url" content="{url}">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta property="og:locale" content="ar_EG">
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="{title_clean}">
    <meta name="twitter:description" content="{desc_short}">
    <meta name="twitter:image" content="{image}">
    """
    return meta

# ================== Local Business Data ==================

def create_google_business_profile_json() -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data JSON Ù„ÙƒÙ„ Ø¯ÙˆÙ„ ÙˆÙ…Ø¯Ù† Ø§Ù„Ø®Ù„ÙŠØ¬"""
    import json
    profiles = []
    
    for country_code, country_data in GULF_COUNTRIES.items():
        for city_name, city_coords in country_data["cities"].items():
            profile = {
                "business_name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {city_name}",
                "country_code": country_code,
                "country_name": country_data['arabic_name'],
                "city": city_name,
                "phone": "+201110760081",
                "website": "https://sherow1982.github.io/arabsad-ads/",
                "latitude": city_coords['lat'],
                "longitude": city_coords['lng'],
                "services": [
                    "Google Ads",
                    "Facebook Ads",
                    "Instagram Ads",
                    "SEO",
                    "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
                    "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ",
                    "ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…ØªØ§Ø¬Ø± Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©"
                ],
                "opening_hours": {
                    "monday": "08:00-23:00",
                    "tuesday": "08:00-23:00",
                    "wednesday": "08:00-23:00",
                    "thursday": "08:00-23:00",
                    "friday": "08:00-23:00",
                    "saturday": "08:00-23:00",
                    "sunday": "08:00-23:00"
                },
                "service_areas": [city_name, country_data['name']],
                "keywords": country_data["keywords"]
            }
            profiles.append(profile)
    
    return json.dumps(profiles, ensure_ascii=False, indent=2)

# ================== Sitemap ==================

def generate_sitemap(all_files: list) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Sitemap XML"""
    sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n'
    sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    for file_path in all_files:
        if file_path.name.endswith('.html'):
            url = build_page_url(file_path)
            try:
                last_mod = datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d')
            except:
                last_mod = datetime.now().strftime('%Y-%m-%d')
            
            if file_path.name == 'index.html':
                priority = "1.0"
                changefreq = "daily"
            elif 'services' in str(file_path):
                priority = "0.8"
                changefreq = "weekly"
            elif 'blog' in str(file_path):
                priority = "0.7"
                changefreq = "weekly"
            else:
                priority = "0.6"
                changefreq = "monthly"
            
            sitemap += f"""  <url>
    <loc>{url}</loc>
    <lastmod>{last_mod}</lastmod>
    <changefreq>{changefreq}</changefreq>
    <priority>{priority}</priority>
  </url>
"""
    
    sitemap += '</urlset>'
    return sitemap

# ================== Robots.txt ==================

def generate_robots_txt() -> str:
    """ØªÙˆÙ„ÙŠØ¯ robots.txt Ù…Ø­Ø³Ù‘Ù†"""
    robots = """User-agent: *
Allow: /
Disallow: /admin/
Disallow: /private/
Disallow: /?*
Disallow: /*?*

User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

Sitemap: https://sherow1982.github.io/arabsad-ads/sitemap.xml
Crawl-delay: 1
"""
    return robots

# ================== Ø§Ù„Ø­Ù‚Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==================

def inject_seo(html: str, title: str, image: str, url: str, description: str, file_path: Path, page_type: str, keywords: list, local_business_schemas: list) -> str:
    """Ø­Ù‚Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ <head>"""
    if '</head>' not in html:
        if '<body' in html.lower():
            html = html.replace('<body', '</head><body', 1)
        else:
            html = html + '</head>'
    
    html = re.sub(
        r'<script\s+type=["\']?application/ld\+json["\']?\s*>.*?</script>',
        '',
        html,
        flags=re.DOTALL | re.IGNORECASE
    )
    
    meta = create_meta_tags(title, image, url, description, keywords)
    
    if page_type == 'article':
        main_schema = create_article_schema(title, image, url, description, file_path)
    else:
        main_schema = create_service_schema(title, image, url, description)
    
    org_schema = create_organization_schema()
    breadcrumb_schema = create_breadcrumb_schema(file_path)
    
    # Ø­Ù‚Ù† 5 Local Business Schemas (Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù)
    local_business_snippets = "\n".join([
        f"<script type=\"application/ld+json\">\n{schema}\n</script>"
        for schema in local_business_schemas[:5]
    ])
    
    injection = f"""
{meta}

<!-- Service/Article Schema JSON-LD (Auto) -->
<script type="application/ld+json">
{main_schema}
</script>

<!-- Organization Schema JSON-LD (Auto) -->
<script type="application/ld+json">
{org_schema}
</script>

<!-- LocalBusiness Schemas - Gulf Countries (Auto) -->
{local_business_snippets}

<!-- Breadcrumb Schema JSON-LD (Auto) -->
<script type="application/ld+json">
{breadcrumb_schema}
</script>

</head>"""
    
    return html.replace('</head>', injection, 1)

def process_file(file_path: Path, all_files: list, local_business_schemas: list) -> tuple:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            html = f.read()
        
        title = extract_title(html)
        image = extract_image(html)
        description = extract_description(html)
        url = build_page_url(file_path)
        page_type = determine_page_type(file_path)
        keywords = extract_page_keywords(file_path, title)
        
        updated = inject_seo(html, title, image, url, description, file_path, page_type, keywords, local_business_schemas)
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(updated)
        
        return (True, file_path.relative_to(Path(".")), page_type, keywords)
    except Exception as e:
        return (False, file_path.relative_to(Path(".")), str(e), [])

def main():
    print("\n" + "="*80)
    print("ğŸ† Ø³ÙƒØ±Ø¨Øª SEO Ø´Ø§Ù…Ù„ + Local Business ÙƒØ§Ù…Ù„ Ø§Ù„Ø®Ù„ÙŠØ¬ - arabsad-ads ğŸ†")
    print("="*80 + "\n")

    root = Path(".")
    
    search_paths = [
        ("root", root, "*.html"),
        ("services", root / "services", "*.html"),
        ("cities", root / "cities", "*.html"),
        ("blog", root / "blog", "*.html"),
        ("articles", root / "blog" / "articles", "*.html"),
    ]
    
    all_files = []
    for folder_name, folder_path, pattern in search_paths:
        if folder_path.exists():
            files = sorted(folder_path.glob(pattern))
            all_files.extend(files)
            if files:
                print(f"ğŸ“‚ {folder_name}: {len(files)} Ù…Ù„Ù")
    
    if not all_files:
        print("\nâŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª HTML")
        sys.exit(1)

    print(f"\nğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(all_files)}\n")
    print("-" * 80 + "\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Local Business Schemas Ø§Ù„ÙƒØ§Ù…Ù„Ø©
    print("ğŸ—ï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Local Business Schemas Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ù†...")
    local_business_schemas = create_local_business_schemas_all()
    print(f"   âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(local_business_schemas)} Local Business Schema\n")

    ok = 0
    fail = 0
    stats = {"service": 0, "article": 0, "city": 0, "blog": 0, "page": 0}
    all_keywords = []

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    for i, fp in enumerate(all_files, 1):
        rel_path = fp.relative_to(root)
        print(f"[{i}/{len(all_files)}] {rel_path} ...", end=" ")
        
        success, filename, result, keywords = process_file(fp, all_files, local_business_schemas)
        if success:
            page_type = result
            stats[page_type] = stats.get(page_type, 0) + 1
            all_keywords.extend(keywords)
            print(f"âœ… ({page_type})")
            ok += 1
        else:
            print(f"âŒ {result}")
            fail += 1

    # Ø¥Ù†Ø´Ø§Ø¡ Sitemap
    print("\nğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Sitemap XML...")
    sitemap_content = generate_sitemap(all_files)
    with open(root / "sitemap.xml", "w", encoding="utf-8") as f:
        f.write(sitemap_content)
    print("   âœ… sitemap.xml ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø¥Ù†Ø´Ø§Ø¡ robots.txt
    print("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ robots.txt...")
    robots_content = generate_robots_txt()
    with open(root / "robots.txt", "w", encoding="utf-8") as f:
        f.write(robots_content)
    print("   âœ… robots.txt ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile JSON
    print("ğŸª Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data...")
    gbp_content = create_google_business_profile_json()
    with open(root / "google-business-profiles.json", "w", encoding="utf-8") as f:
        f.write(gbp_content)
    total_profiles = gbp_content.count('"business_name"')
    print(f"   âœ… google-business-profiles.json ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ ({total_profiles} Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ)")

    # Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print("\n" + "="*80)
    print("ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print("="*80)
    print(f"âœ… Ù…Ù„ÙØ§Øª Ù…Ø­Ø¯Ø«Ø©: {ok}")
    print(f"âŒ Ù…Ù„ÙØ§Øª ÙØ´Ù„Øª: {fail}")
    print(f"ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {(ok/len(all_files)*100):.1f}%")
    print("\nğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹:")
    for page_type, count in stats.items():
        if count > 0:
            print(f"   â€¢ {page_type}: {count} Ù…Ù„Ù")

    print("\nğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:")
    print("   âœ… sitemap.xml - Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ XML")
    print("   âœ… robots.txt - ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«")
    print(f"   âœ… google-business-profiles.json - {total_profiles} Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ ØªØ¬Ø§Ø±ÙŠ")

    print("\nğŸŒ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ Ø§Ù„Ù…ÙØ¯Ø¹ÙˆÙ…Ø© ÙƒØ§Ù…Ù„Ø©:")
    total_cities = 0
    for code, country in GULF_COUNTRIES.items():
        city_count = len(country["cities"])
        total_cities += city_count
        cities_list = ", ".join(list(country["cities"].keys())[:3]) + "..."
        print(f"   â€¢ {country['name']} ({code}): {city_count} Ù…Ø¯ÙŠÙ†Ø© - {cities_list}")
    print(f"\n   ğŸ’° Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¯Ù†: {total_cities} Ù…Ø¯ÙŠÙ†Ø© ÙÙŠ {len(GULF_COUNTRIES)} Ø¯ÙˆÙ„")

    print("\n" + "="*80)
    print("âœ¨ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø­Ù„ SEO Ù…ØªÙƒØ§Ù…Ù„ Ø´Ø§Ù…Ù„!")
    print("="*80)
    print("\nğŸ“ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:")
    print("1. Ø§Ø±ÙØ¹ sitemap.xml Ùˆ robots.txt Ø¹Ù„Ù‰ GitHub")
    print("2. Ø§Ø¯Ø®Ù„ Google Search Console ÙˆØ£Ø¶Ù sitemap.xml")
    print("3. Ø§Ø³ØªØ®Ø¯Ù… google-business-profiles.json Ù„Ø¥Ù†Ø´Ø§Ø¡ Google Business Profiles")
    print("4. Ø§Ø±Ø¨Ø· Ø§Ù„Ù…Ù„Ù google-business-profiles.json Ø¨Ù€ Google Business Profile API")
    print("5. Ø­Ø³Ù‘Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù€ 1500+ ÙƒÙ„Ù…Ø© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Keywords")
    print("\n")

if __name__ == "__main__":
    main()
