#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø³ÙƒØ±Ø¨Øª Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù€ SEO ÙˆLocal Business Ùˆ Content Optimization
Ø±ÙŠØ¨Ùˆ: arabsad-ads (Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨)
Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ ÙƒØ§Ù…Ù„Ø© + ÙƒÙ„ Ù…Ø¯Ù†Ù‡Ø§
"""

import sys
import re
import json
from pathlib import Path
from datetime import datetime, timedelta

# ================== Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ ÙƒØ§Ù…Ù„Ø© ==================

GULF_COUNTRIES = {
    "SA": {
        "name": "Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "arabic_name": "Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "lat": 24.7136,
        "lng": 46.6753,
        "cities": {
            "Ø§Ù„Ø±ÙŠØ§Ø¶": {"lat": 24.7136, "lng": 46.6753},
            "Ø¬Ø¯Ø©": {"lat": 21.5485, "lng": 39.1721},
            "Ø§Ù„Ø¯Ù…Ø§Ù…": {"lat": 26.3989, "lng": 50.2048},
            "Ø§Ù„Ø®Ø¨Ø±": {"lat": 26.2156, "lng": 50.2106},
            "Ø§Ù„Ù‚Ø·ÙŠÙ": {"lat": 26.1801, "lng": 50.0157},
            "Ù…ÙƒØ©": {"lat": 21.4225, "lng": 39.8262},
            "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©": {"lat": 24.4647, "lng": 39.6074},
            "Ø§Ù„Ø·Ø§Ø¦Ù": {"lat": 21.2745, "lng": 40.4158},
            "ØªØ¨ÙˆÙƒ": {"lat": 28.3852, "lng": 36.5627},
            "Ø£Ø¨Ù‡Ø§": {"lat": 18.2155, "lng": 42.5054},
            "Ø¬ÙŠØ²Ø§Ù†": {"lat": 16.8892, "lng": 42.5521},
            "Ù†Ø¬Ø±Ø§Ù†": {"lat": 17.6927, "lng": 44.1860},
            "Ø­ÙØ± Ø§Ù„Ø¨Ø§Ø·Ù†": {"lat": 28.4347, "lng": 45.3569},
        },
    },
    "AE": {
        "name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª",
        "arabic_name": "Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©",
        "lat": 23.4241,
        "lng": 53.8478,
        "cities": {
            "Ø¯Ø¨ÙŠ": {"lat": 25.2048, "lng": 55.2708},
            "Ø£Ø¨ÙˆØ¸Ø¨ÙŠ": {"lat": 24.4539, "lng": 54.3773},
            "Ø§Ù„Ø´Ø§Ø±Ù‚Ø©": {"lat": 25.3548, "lng": 55.3944},
            "Ø¹Ø¬Ù…Ø§Ù†": {"lat": 25.3986, "lng": 55.4501},
            "Ø£Ù… Ø§Ù„Ù‚ÙŠÙˆÙŠÙ†": {"lat": 25.5645, "lng": 55.5597},
            "Ø±Ø£Ø³ Ø§Ù„Ø®ÙŠÙ…Ø©": {"lat": 25.7482, "lng": 55.9754},
            "Ø§Ù„ÙØ¬ÙŠØ±Ø©": {"lat": 25.1242, "lng": 56.3540},
        },
    },
    "KW": {
        "name": "Ø§Ù„ÙƒÙˆÙŠØª",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ø§Ù„ÙƒÙˆÙŠØª",
        "lat": 29.3759,
        "lng": 47.9774,
        "cities": {
            "Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„ÙƒÙˆÙŠØª": {"lat": 29.3759, "lng": 47.9774},
            "Ø§Ù„Ø£Ø­Ù…Ø¯ÙŠ": {"lat": 29.1118, "lng": 47.6929},
            "Ø§Ù„Ø¬Ù‡Ø±Ø§Ø¡": {"lat": 29.4444, "lng": 47.6804},
            "Ø§Ù„ÙØ±ÙˆØ§Ù†ÙŠØ©": {"lat": 29.2269, "lng": 47.8558},
            "Ø­ÙˆÙ„ÙŠ": {"lat": 29.3621, "lng": 47.9825},
            "Ù…Ø¨Ø§Ø±Ùƒ Ø§Ù„ÙƒØ¨ÙŠØ±": {"lat": 29.0269, "lng": 47.7373},
            "Ø§Ù„Ø¹Ø§ØµÙ…Ø©": {"lat": 29.3759, "lng": 47.9774},
        },
    },
    "QA": {
        "name": "Ù‚Ø·Ø±",
        "arabic_name": "Ø¯ÙˆÙ„Ø© Ù‚Ø·Ø±",
        "lat": 25.2854,
        "lng": 51.5310,
        "cities": {
            "Ø§Ù„Ø¯ÙˆØ­Ø©": {"lat": 25.2854, "lng": 51.5310},
            "Ø§Ù„Ø±ÙŠØ§Ù†": {"lat": 25.3548, "lng": 51.5342},
            "Ø§Ù„ÙˆÙƒØ±Ø©": {"lat": 25.1673, "lng": 51.6286},
            "Ø§Ù„Ø®ÙˆØ±": {"lat": 25.6753, "lng": 51.4805},
            "Ø£Ù… ØµÙ„Ø§Ù„": {"lat": 25.4167, "lng": 51.5000},
            "Ø§Ù„Ø´Ù…Ø§Ù„": {"lat": 25.8500, "lng": 51.2500},
        },
    },
    "BH": {
        "name": "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "arabic_name": "Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†",
        "lat": 26.0667,
        "lng": 50.5577,
        "cities": {
            "Ø§Ù„Ù…Ù†Ø§Ù…Ø©": {"lat": 26.1290, "lng": 50.5826},
            "Ø§Ù„Ù…Ø­Ø±Ù‚": {"lat": 26.1667, "lng": 50.5833},
            "Ø§Ù„Ø±ÙØ§Ø¹": {"lat": 26.1333, "lng": 50.4167},
            "Ø§Ù„Ø¬ÙÙŠØ±": {"lat": 26.1778, "lng": 50.4389},
            "Ø³Ù„Ù…Ø§Ù† Ø¢Ø¨Ø§Ø¯": {"lat": 26.0833, "lng": 50.5000},
        },
    },
    "OM": {
        "name": "Ø¹Ù…Ø§Ù†",
        "arabic_name": "Ø³Ù„Ø·Ù†Ø© Ø¹Ù…Ø§Ù†",
        "lat": 21.4735,
        "lng": 55.9754,
        "cities": {
            "Ù…Ø³Ù‚Ø·": {"lat": 21.4735, "lng": 55.9754},
            "ØµÙ„Ø§Ù„Ø©": {"lat": 17.0151, "lng": 54.0924},
            "ØµØ­Ø§Ø±": {"lat": 24.2795, "lng": 56.9366},
            "Ù†Ø²ÙˆÙ‰": {"lat": 22.9342, "lng": 57.5364},
            "Ø§Ù„Ø³ÙˆÙŠÙ‚": {"lat": 23.8069, "lng": 57.4074},
            "Ø´Ù†Ø§Øµ": {"lat": 24.7167, "lng": 56.7833},
            "Ù‡ÙŠÙ…Ø§Ø¡": {"lat": 24.2000, "lng": 56.6000},
        },
    },
}

# ================== Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ==================

def extract_title(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"""
    m = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt.split('|')[0].strip() if '|' in txt else txt
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    return "ØµÙØ­Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_description(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ"""
    m = re.search(r'<meta\s+name=["\']description["\']\s+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    m = re.search(r'<p[^>]*>([^<]+)</p>', html, re.IGNORECASE)
    if m:
        txt = m.group(1).strip()
        return txt if len(txt) <= 155 else txt[:152] + "..."
    return "Ø®Ø¯Ù…Ø§Øª ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªÙ…ÙŠØ²Ø© Ù…Ù† Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"

def extract_image(html: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø©"""
    m = re.search(r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>', html, re.IGNORECASE)
    if m:
        src = m.group(1).strip()
        if src.startswith('http'):
            return src
        src = src.lstrip('./')
        return f"https://sherow1982.github.io/arabsad-ads/{src}"
    return "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"

def determine_page_type(file_path: Path) -> str:
    """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©"""
    relative = str(file_path.relative_to(Path("."))).lower()
    if 'blog/articles' in relative:
        return 'article'
    elif 'blog' in relative:
        return 'blog'
    elif 'services' in relative:
        return 'service'
    elif 'cities' in relative:
        return 'city'
    return 'page'

def build_page_url(file_path: Path) -> str:
    """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø·"""
    relative_path = file_path.relative_to(Path("."))
    url_path = str(relative_path).replace("\\", "/")
    return f"https://sherow1982.github.io/arabsad-ads/{url_path}"

def extract_page_keywords(file_path: Path, title: str) -> list:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ keywords"""
    keywords = []
    
    # Global keywords
    keywords.extend([
        "Google Ads", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„", "Facebook Ads", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙÙŠØ³Ø¨ÙˆÙƒ",
        "SEO", "ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«", "ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ", "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ",
        "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹", "Web Design", "Social Media Ads", "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„"
    ])
    
    # Country keywords
    for country_code, country_data in GULF_COUNTRIES.items():
        keywords.append(f"ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ {country_data['name']}")
        keywords.append(f"Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø¬ÙˆØ¬Ù„ {country_data['name']}")
        for city in list(country_data["cities"].keys())[:2]:
            keywords.append(f"ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ {city}")
            keywords.append(f"Google Ads {city}")
    
    keywords.append(title)
    return list(set(keywords))[:25]

# ================== Schema ==================

def create_service_schema(title: str, image: str, url: str, description: str) -> str:
    """Service Schema"""
    import json
    
    area_served = []
    for country_code, country_data in GULF_COUNTRIES.items():
        area_served.append({"@type": "Country", "name": country_data['arabic_name']})
    
    schema = {
        "@context": "https://schema.org/",
        "@type": "Service",
        "name": title,
        "image": image,
        "description": description,
        "provider": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "url": "https://sherow1982.github.io/arabsad-ads/",
            "logo": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
            "telephone": "+201110760081"
        },
        "url": url,
        "areaServed": area_served,
        "priceRange": "$$-$$$"
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_article_schema(title: str, image: str, url: str, description: str, file_path: Path) -> str:
    """Article Schema"""
    import json
    try:
        date_modified = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
    except:
        date_modified = datetime.now().isoformat()
    
    schema = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": title,
        "image": image,
        "description": description,
        "datePublished": date_modified,
        "dateModified": date_modified,
        "author": {"@type": "Organization", "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨"},
        "publisher": {
            "@type": "Organization",
            "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
            "logo": {"@type": "ImageObject", "url": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg"}
        },
        "url": url
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_organization_schema() -> str:
    """Organization Schema"""
    import json
    schema = {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨",
        "alternateName": "ArabSad Digital Marketing",
        "image": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
        "description": "ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ù…ØªØ®ØµØµØ© ÙÙŠ Google Ads ÙˆFacebook Ads ÙˆSEO ÙˆØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹",
        "url": "https://sherow1982.github.io/arabsad-ads/",
        "telephone": "+201110760081",
        "email": "info@arabsad.com",
        "address": {
            "@type": "PostalAddress",
            "addressCountry": "EG",
            "addressRegion": "Ø§Ù„Ø¬ÙŠØ²Ø©",
            "addressLocality": "Ø­Ø¯Ø§Ø¦Ù‚ Ø£ÙƒØªÙˆØ¨Ø±"
        }
    }
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_breadcrumb_schema(file_path: Path) -> str:
    """Breadcrumb Schema"""
    import json
    relative = file_path.relative_to(Path("."))
    parts = relative.parts
    breadcrumb_items = [{"@type": "ListItem", "position": 1, "name": "Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "item": "https://sherow1982.github.io/arabsad-ads"}]
    
    current_path = ""
    for i, part in enumerate(parts[:-1], start=2):
        current_path += f"/{part}" if current_path else part
        name = part.replace('-', ' ').title()
        breadcrumb_items.append({
            "@type": "ListItem",
            "position": i,
            "name": name,
            "item": f"https://sherow1982.github.io/arabsad-ads/{current_path}"
        })
    
    schema = {"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": breadcrumb_items}
    return json.dumps(schema, ensure_ascii=False, indent=2)

def create_meta_tags(title: str, image: str, url: str, description: str, keywords: list) -> str:
    """Meta Tags"""
    if len(description) > 155:
        desc_short = description[:152] + "..."
    else:
        desc_short = description
    
    title_clean = title.replace('"', '').replace("'", '')
    keywords_str = ", ".join(keywords[:15])
    
    meta = f"""
    <!-- SEO Meta Tags (Auto) -->
    <meta charset="UTF-8">
    <title>{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ | ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø®Ù„ÙŠØ¬</title>
    <meta name="description" content="{desc_short}">
    <meta name="keywords" content="{keywords_str}">
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="language" content="ar">
    <meta name="author" content="Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <link rel="canonical" href="{url}">
    <!-- Open Graph -->
    <meta property="og:title" content="{title_clean} - Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨">
    <meta property="og:description" content="{desc_short}">
    <meta property="og:image" content="{image}">
    <meta property="og:url" content="{url}">
    <meta property="og:type" content="website">
    <meta property="og:locale" content="ar_EG">
    """
    return meta

# ================== Google Business Profiles ==================

def create_google_business_profile_json() -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data JSON Ù„ÙƒÙ„ Ø§Ù„Ù…Ø¯Ù†"""
    import json
    profiles = []
    
    for country_code, country_data in GULF_COUNTRIES.items():
        for city_name, city_coords in country_data["cities"].items():
            profile = {
                "business_name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {city_name}",
                "country_code": country_code,
                "country_name": country_data['arabic_name'],
                "city": city_name,
                "phone": "+201110760081",
                "website": "https://sherow1982.github.io/arabsad-ads/",
                "latitude": city_coords['lat'],
                "longitude": city_coords['lng'],
                "services": [
                    "Google Ads", "Facebook Ads", "Instagram Ads", "SEO",
                    "ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹", "Ø§Ù„ØªØ³ÙˆÙŠÙ‚ Ø§Ù„Ø±Ù‚Ù…ÙŠ"
                ],
                "opening_hours": {
                    "monday": "08:00-23:00", "tuesday": "08:00-23:00",
                    "wednesday": "08:00-23:00", "thursday": "08:00-23:00",
                    "friday": "08:00-23:00", "saturday": "08:00-23:00",
                    "sunday": "08:00-23:00"
                },
                "service_areas": [city_name, country_data['name']]
            }
            profiles.append(profile)
    
    # Ø­ÙØ¸ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
    json_str = json.dumps(profiles, ensure_ascii=False, indent=2)
    return json_str

def create_local_business_schemas_all() -> list:
    """LocalBusiness Schema Ù„ÙƒÙ„ Ø§Ù„Ù…Ø¯Ù†"""
    import json
    schemas = []
    
    for country_code, country_data in GULF_COUNTRIES.items():
        for city_name, city_coords in country_data["cities"].items():
            schema = {
                "@context": "https://schema.org",
                "@type": "LocalBusiness",
                "name": f"Ù…Ø¤Ø³Ø³Ø© Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ - {city_name}",
                "image": "https://sherow1982.github.io/arabsad-ads/assets/images/logo.svg",
                "url": "https://sherow1982.github.io/arabsad-ads/",
                "telephone": "+201110760081",
                "address": {
                    "@type": "PostalAddress",
                    "addressCountry": country_code,
                    "addressLocality": city_name
                },
                "geo": {
                    "@type": "GeoCoordinates",
                    "latitude": city_coords['lat'],
                    "longitude": city_coords['lng']
                }
            }
            schemas.append(json.dumps(schema, ensure_ascii=False, indent=2))
    
    return schemas

# ================== Sitemap ==================

def generate_sitemap(all_files: list) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Sitemap XML"""
    sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    for file_path in all_files:
        if file_path.name.endswith('.html'):
            url = build_page_url(file_path)
            try:
                last_mod = datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d')
            except:
                last_mod = datetime.now().strftime('%Y-%m-%d')
            
            priority = "1.0" if file_path.name == 'index.html' else "0.7"
            changefreq = "daily" if file_path.name == 'index.html' else "weekly"
            
            sitemap += f"""  <url>
    <loc>{url}</loc>
    <lastmod>{last_mod}</lastmod>
    <changefreq>{changefreq}</changefreq>
    <priority>{priority}</priority>
  </url>
"""
    
    sitemap += '</urlset>'
    return sitemap

# ================== Robots.txt ==================

def generate_robots_txt() -> str:
    """ØªÙˆÙ„ÙŠØ¯ robots.txt"""
    return """User-agent: *
Allow: /
Disallow: /admin/
Disallow: /private/

User-agent: Googlebot
Allow: /

Sitemap: https://sherow1982.github.io/arabsad-ads/sitemap.xml
Crawl-delay: 1
"""

# ================== Ø§Ù„Ø­Ù‚Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==================

def inject_seo(html: str, title: str, image: str, url: str, description: str, file_path: Path, page_type: str, keywords: list, local_business_schemas: list) -> str:
    """Ø­Ù‚Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ <head>"""
    if '</head>' not in html:
        if '<body' in html.lower():
            html = html.replace('<body', '</head><body', 1)
        else:
            html = html + '</head>'
    
    html = re.sub(r'<script\s+type=["\']?application/ld\+json["\']?\s*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    
    meta = create_meta_tags(title, image, url, description, keywords)
    
    if page_type == 'article':
        main_schema = create_article_schema(title, image, url, description, file_path)
    else:
        main_schema = create_service_schema(title, image, url, description)
    
    org_schema = create_organization_schema()
    breadcrumb_schema = create_breadcrumb_schema(file_path)
    
    local_business_snippets = "\n".join([f"<script type=\"application/ld+json\">\n{schema}\n</script>" for schema in local_business_schemas[:10]])
    
    injection = f"""
{meta}

<!-- Main Schema (Auto) -->
<script type="application/ld+json">
{main_schema}
</script>

<!-- Organization Schema (Auto) -->
<script type="application/ld+json">
{org_schema}
</script>

<!-- LocalBusiness Schemas - Gulf Countries (Auto) -->
{local_business_snippets}

<!-- Breadcrumb Schema (Auto) -->
<script type="application/ld+json">
{breadcrumb_schema}
</script>

</head>"""
    
    return html.replace('</head>', injection, 1)

def process_file(file_path: Path, all_files: list, local_business_schemas: list) -> tuple:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            html = f.read()
        
        title = extract_title(html)
        image = extract_image(html)
        description = extract_description(html)
        url = build_page_url(file_path)
        page_type = determine_page_type(file_path)
        keywords = extract_page_keywords(file_path, title)
        
        updated = inject_seo(html, title, image, url, description, file_path, page_type, keywords, local_business_schemas)
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(updated)
        
        return (True, file_path.relative_to(Path(".")), page_type)
    except Exception as e:
        return (False, file_path.relative_to(Path(".")), str(e))

def main():
    print("\n" + "="*80)
    print("ğŸ† Ø³ÙƒØ±Ø¨Øª SEO Ø´Ø§Ù…Ù„ + Local Business ÙƒØ§Ù…Ù„ Ø§Ù„Ø®Ù„ÙŠØ¬ - arabsad-ads ğŸ†")
    print("="*80 + "\n")

    root = Path(".")
    
    search_paths = [
        ("root", root, "*.html"),
        ("services", root / "services", "*.html"),
        ("cities", root / "cities", "*.html"),
        ("blog", root / "blog", "*.html"),
        ("articles", root / "blog" / "articles", "*.html"),
    ]
    
    all_files = []
    for folder_name, folder_path, pattern in search_paths:
        if folder_path.exists():
            files = sorted(folder_path.glob(pattern))
            all_files.extend(files)
            if files:
                print(f"ğŸ“‚ {folder_name}: {len(files)} Ù…Ù„Ù")
    
    if not all_files:
        print("\nâŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª HTML")
        sys.exit(1)

    print(f"\nğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(all_files)}\n")

    # Ø¥Ù†Ø´Ø§Ø¡ Local Business Schemas
    print("ğŸ—ï¸ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Local Business Schemas...")
    local_business_schemas = create_local_business_schemas_all()
    print(f"   âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(local_business_schemas)} Local Business Schema\n")

    ok = 0
    fail = 0

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    for i, fp in enumerate(all_files, 1):
        rel_path = fp.relative_to(root)
        print(f"[{i}/{len(all_files)}] {rel_path} ...", end=" ")
        
        success, filename, result = process_file(fp, all_files, local_business_schemas)
        if success:
            print(f"âœ…")
            ok += 1
        else:
            print(f"âŒ {result}")
            fail += 1

    # Ø¥Ù†Ø´Ø§Ø¡ Sitemap
    print("\nğŸ“ Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Sitemap XML...")
    sitemap_content = generate_sitemap(all_files)
    with open(root / "sitemap.xml", "w", encoding="utf-8") as f:
        f.write(sitemap_content)
    print("   âœ… sitemap.xml ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø¥Ù†Ø´Ø§Ø¡ robots.txt
    print("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ robots.txt...")
    robots_content = generate_robots_txt()
    with open(root / "robots.txt", "w", encoding="utf-8") as f:
        f.write(robots_content)
    print("   âœ… robots.txt ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§")

    # Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile JSON
    print("ğŸª Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Google Business Profile Data...")
    gbp_content = create_google_business_profile_json()
    gbp_file_path = root / "google-business-profiles.json"
    
    with open(gbp_file_path, "w", encoding="utf-8") as f:
        f.write(gbp_content)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª
    gbp_count = gbp_content.count('"business_name"')
    print(f"   âœ… google-business-profiles.json ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ ({gbp_count} Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ)\n")
    
    # Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print("="*80)
    print("ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print("="*80)
    print(f"âœ… Ù…Ù„ÙØ§Øª Ù…Ø­Ø¯Ø«Ø©: {ok}")
    print(f"âŒ Ù…Ù„ÙØ§Øª ÙØ´Ù„Øª: {fail}")
    print(f"ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­: {(ok/len(all_files)*100):.1f}%")

    print("\nğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:")
    print("   âœ… sitemap.xml")
    print("   âœ… robots.txt")
    print(f"   âœ… google-business-profiles.json ({gbp_count} Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ)")

    print("\nğŸŒ Ø¯ÙˆÙ„ Ø§Ù„Ø®Ù„ÙŠØ¬ Ø§Ù„Ù…ÙØ¯Ø¹ÙˆÙ…Ø©:")
    total_cities = 0
    for code, country in GULF_COUNTRIES.items():
        city_count = len(country["cities"])
        total_cities += city_count
        print(f"   âœ… {country['name']} ({code}): {city_count} Ù…Ø¯ÙŠÙ†Ø©")
    
    print(f"\n   ğŸ’° Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_cities} Ù…Ø¯ÙŠÙ†Ø© ÙÙŠ {len(GULF_COUNTRIES)} Ø¯ÙˆÙ„")
    print(f"   ğŸ’° Ø¥Ø¬Ù…Ø§Ù„ÙŠ Google Business Profiles: {gbp_count}")

    print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()
